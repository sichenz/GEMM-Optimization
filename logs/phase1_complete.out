========================================================================
                    PHASE 1: COMPLETE BASELINE ANALYSIS                
========================================================================

This script will:
  1. Collect GPU specifications
  2. Run GEMM benchmarks (Lab-1, cuBLAS)
  3. Profile Lab-1 kernel with Nsight Compute
  4. Run CUTLASS benchmarks
  5. Generate comprehensive analysis and visualizations

Wed Nov 12 08:20:48 PM EST 2025

========================================================================
STEP 1: Building Project
========================================================================
-- The CXX compiler identification is GNU 13.3.0
-- The CUDA compiler identification is NVIDIA 12.8.93
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version "12.8.93") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Configuring done (4.8s)
-- Generating done (0.5s)
-- Build files have been written to: /scratch/sz4972/GEMM-Optimization/build
[ 16%] Building CUDA object CMakeFiles/gpu_specs.dir/src/gpu_specs.cu.o
[ 50%] Building CUDA object CMakeFiles/benchmark_gemm.dir/src/benchmark_gemm.cu.o
[ 50%] Building CUDA object CMakeFiles/cublas_bench.dir/src/baselines/cublas_bench.cu.o
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 66%] Linking CUDA executable gpu_specs
[ 66%] Built target gpu_specs
[ 83%] Linking CUDA executable cublas_bench
[ 83%] Built target cublas_bench
[100%] Linking CUDA executable benchmark_gemm
[100%] Built target benchmark_gemm
✓ Build successful

========================================================================
STEP 2: GPU Specifications
========================================================================
========================================
GPU Specifications Report
Generated: Nov 12 2025 20:21:08
========================================

========================================
Device 0: Quadro RTX 8000
========================================

=== Basic Information ===
Compute Capability: 7.5
Total Global Memory: 44.48 GB
Memory Clock Rate: 6501.00 MHz
Memory Bus Width: 384 bits

=== Compute Resources ===
Number of SMs: 72
CUDA Cores: 4608 (total)
CUDA Cores per SM: 64
Max Threads per SM: 1024
Max Threads per Block: 1024
Max Block Dimensions: (1024, 1024, 64)
Max Grid Dimensions: (2147483647, 65535, 65535)
Warp Size: 32

=== Memory Hierarchy ===
L2 Cache Size: 6.00 MB
Shared Memory per Block: 48.00 KB
Shared Memory per SM: 64.00 KB
Registers per Block: 65536
Registers per SM: 65536
Constant Memory: 64.00 KB

=== Performance Characteristics ===
Peak Memory Bandwidth: 624.1 GB/s
Estimated FP32 Peak TFLOPS: 14.93 TFLOPS
Base Clock Rate: 1.62 GHz
TensorCore Architecture: Turing
TensorCore FP16 Peak TFLOPS: 65.0 TFLOPS
TensorCore Speedup vs FP32: 4.4x

=== Feature Support ===
Concurrent Kernels: Yes
ECC Enabled: Yes
Unified Addressing: Yes
Managed Memory: Yes
Cooperative Launch: Yes
TensorCore Support: Yes (Compute 7.5)

=== Arithmetic Intensity Analysis ===
FP32 Ridge Point: 23.92 FLOPS/Byte
  (Workloads with AI > 23.92 are compute-bound)
FP16 TensorCore Ridge Point: 104.15 FLOPS/Byte

✓ Results saved to results/gpu_specs.txt
✓ GPU specs collected

========================================================================
STEP 3: GEMM Benchmarks (Lab-1, cuBLAS)
========================================================================
This may take 30-60 minutes...

GEMM Benchmark Suite
====================

GPU: Quadro RTX 8000
Compute Capability: 7.5

         Kernel   DType     M     N     K    Time(ms)      GFLOPSBandwidth(GB/s)
--------------------------------------------------------------------------------

Testing M=128, N=128, K=128
     Lab1_Tiled    FP32   128   128   128       0.014      298.64          14.00
   cuBLAS_SGEMM    FP32   128   128   128       0.008      509.81          23.90
cuBLAS_HGEMM_TensorCore    FP16   128   128   128       0.007      613.35          14.38

Testing M=256, N=256, K=256
     Lab1_Tiled    FP32   256   256   256       0.028     1199.68          28.12
   cuBLAS_SGEMM    FP32   256   256   256       0.012     2707.40          63.45
cuBLAS_HGEMM_TensorCore    FP16   256   256   256       0.009     3546.68          41.56

Testing M=512, N=512, K=512
     Lab1_Tiled    FP32   512   512   512       0.197     1365.63          16.00
   cuBLAS_SGEMM    FP32   512   512   512       0.042     6456.00          75.66
cuBLAS_HGEMM_TensorCore    FP16   512   512   512       0.018    14751.79          86.44

Testing M=1024, N=1024, K=1024
     Lab1_Tiled    FP32  1024  1024  1024       1.461     1469.40           8.61
   cuBLAS_SGEMM    FP32  1024  1024  1024       0.237     9055.00          53.06
cuBLAS_HGEMM_TensorCore    FP16  1024  1024  1024       0.040    53364.77         156.34

Testing M=2048, N=2048, K=2048
     Lab1_Tiled    FP32  2048  2048  2048       9.231     1861.04           5.45
   cuBLAS_SGEMM    FP32  2048  2048  2048       1.315    13066.08          38.28
cuBLAS_HGEMM_TensorCore    FP16  2048  2048  2048       0.191    90026.15         131.87

Testing M=4096, N=4096, K=4096
     Lab1_Tiled    FP32  4096  4096  4096      70.373     1953.00           2.86
   cuBLAS_SGEMM    FP32  4096  4096  4096      10.656    12898.04          18.89
cuBLAS_HGEMM_TensorCore    FP16  4096  4096  4096       1.562    88013.96          64.46

Testing M=8192, N=8192, K=8192
     Lab1_Tiled    FP32  8192  8192  8192     563.432     1951.45           1.43
   cuBLAS_SGEMM    FP32  8192  8192  8192      84.820    12962.95           9.49
cuBLAS_HGEMM_TensorCore    FP16  8192  8192  8192      12.462    88229.37          32.31

Testing M=4096, N=256, K=1024
     Lab1_Tiled    FP32  4096   256  1024       1.277     1682.10          17.25
   cuBLAS_SGEMM    FP32  4096   256  1024       0.191    11219.69         115.05
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.040    54013.33         276.92

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.289     1876.75          11.91
   cuBLAS_SGEMM    FP32  1024  4096   512       0.355    12110.45          76.87
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.065    66006.56         209.49

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.303     1864.82          10.93
   cuBLAS_SGEMM    FP32  2048   512  2048       0.353    12153.33          71.21
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    73238.97         214.57

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.566     1897.76          16.68
   cuBLAS_SGEMM    FP32   512  2048   512       0.094    11458.09         100.71
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.019    56223.91         247.08
Results saved to results/benchmark_results.csv

Benchmark completed!
✓ Benchmarks complete

========================================================================
STEP 4: Nsight Compute Profiling
========================================================================
Profiling Lab-1 kernel for key matrix sizes...

Building profiling executable...
-- The CXX compiler identification is GNU 13.3.0
-- The CUDA compiler identification is NVIDIA 12.8.93
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version "12.8.93") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Configuring done (2.3s)
-- Generating done (0.0s)
-- Build files have been written to: /scratch/sz4972/GEMM-Optimization/build_profile
[ 50%] Building CUDA object CMakeFiles/benchmark_gemm.dir/src/benchmark_gemm.cu.o
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[100%] Linking CUDA executable benchmark_gemm
[100%] Built target benchmark_gemm
Profiling 512x512x512...
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.040    53554.28         274.57

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.300     1867.26          11.85
   cuBLAS_SGEMM    FP32  1024  4096   512       0.350    12267.35          77.87
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.066    65211.22         206.97

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.314     1856.28          10.88
   cuBLAS_SGEMM    FP32  2048   512  2048       0.355    12115.26          70.99
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    72723.08         213.06

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.575     1868.56          16.42
   cuBLAS_SGEMM    FP32   512  2048   512       0.095    11361.10          99.85
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.030    35869.83         157.63
Results saved to results/benchmark_results.csv

Benchmark completed!
==PROF== Disconnected from process 1384974
  ✓ Profiling 512 succeeded
Profiling 2048x2048x2048...
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.040    53473.20         274.15

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.309     1859.81          11.81
   cuBLAS_SGEMM    FP32  1024  4096   512       0.351    12245.03          77.73
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.066    65059.49         206.49

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.318     1853.21          10.86
   cuBLAS_SGEMM    FP32  2048   512  2048       0.354    12146.07          71.17
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    73015.84         213.91

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.573     1874.85          16.48
   cuBLAS_SGEMM    FP32   512  2048   512       0.094    11391.57         100.12
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.030    35505.46         156.03
Results saved to results/benchmark_results.csv

Benchmark completed!
==PROF== Disconnected from process 1385210
  ✓ Profiling 2048 succeeded
Profiling 4096x4096x4096...
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.040    53567.10         274.64

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.303     1865.16          11.84
   cuBLAS_SGEMM    FP32  1024  4096   512       0.351    12229.52          77.63
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.066    65051.61         206.46

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.316     1854.37          10.87
   cuBLAS_SGEMM    FP32  2048   512  2048       0.354    12136.84          71.11
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    72701.42         212.99

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.575     1866.26          16.40
   cuBLAS_SGEMM    FP32   512  2048   512       0.094    11409.19         100.28
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.030    35419.26         155.65
Results saved to results/benchmark_results.csv

Benchmark completed!
==PROF== Disconnected from process 1385360
  ✓ Profiling 4096 succeeded
✓ Profiling step complete

========================================================================
STEP 5: CUTLASS Benchmarks
========================================================================
Initializing CUTLASS submodule...
Detected GPU compute capability: 75
Running CUTLASS benchmarks...
          cuBLAS: Not run
           cuDNN: Not run

       Arguments: --gemm_kind=universal --m=8192 --n=8192 --k=8192 --A=f16:row --B=f16:row --C=f16:column --D=f16:column  \
                  --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1 --batch_count=1 --raster_order=heuristic  \
                  --runtime_input_datatype_a=invalid --runtime_input_datatype_b=invalid --use_pdl=false --enable_sm90_mixed_dtype_shuffle_test=false  \
                  --swizzle_size=1 --op_class=tensorop --accum=f32 --cta_m=256 --cta_n=128 --cta_k=32 --cluster_m=1 --cluster_n=1  \
                  --cluster_k=1 --cluster_m_fallback=0 --cluster_n_fallback=0 --cluster_k_fallback=0 --stages=2 --warps_m=4  \
                  --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=8 --min_cc=75 --max_cc=1024

           Bytes: 402653184  bytes
           FLOPs: 1099645845504  flops
           FLOPs/Byte: 2731

         Runtime: 11.9445  ms
          Memory: 31.3951 GiB/s

            Math: 92062.8 GFLOP/s

Wrote results to 'results/cutlass/cutlass_fp32.gemm.csv'
✓ FP32 benchmarks complete
          cuBLAS: Not run
           cuDNN: Not run

       Arguments: --gemm_kind=universal --m=8192 --n=8192 --k=8192 --A=f16:row --B=f16:row --C=f16:column --D=f16:column  \
                  --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1 --batch_count=1 --raster_order=heuristic  \
                  --runtime_input_datatype_a=invalid --runtime_input_datatype_b=invalid --use_pdl=false --enable_sm90_mixed_dtype_shuffle_test=false  \
                  --swizzle_size=1 --op_class=tensorop --accum=f32 --cta_m=256 --cta_n=128 --cta_k=32 --cluster_m=1 --cluster_n=1  \
                  --cluster_k=1 --cluster_m_fallback=0 --cluster_n_fallback=0 --cluster_k_fallback=0 --stages=2 --warps_m=4  \
                  --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=8 --min_cc=75 --max_cc=1024

           Bytes: 402653184  bytes
           FLOPs: 1099645845504  flops
           FLOPs/Byte: 2731

         Runtime: 11.9553  ms
          Memory: 31.3669 GiB/s

            Math: 91980 GFLOP/s

Wrote results to 'results/cutlass/cutlass_f16tc.gemm.csv'
✓ FP16 TensorCore benchmarks complete
✓ CUTLASS benchmarks complete

========================================================================
STEP 6: Generating Analysis and Visualizations
========================================================================
Parsed GPU specs from file:
  FP32 Peak: 14930.0 GFLOPS
  FP16 Peak: 65000.0 GFLOPS
  Bandwidth: 624.1 GB/s

Loaded 33 benchmark results
Kernels tested: Lab1_Tiled, cuBLAS_SGEMM, cuBLAS_HGEMM_TensorCore
Matrix sizes: 11

Generating analysis...
Roofline plot saved to results/roofline_plot.png
Performance comparison saved to results/performance_comparison.png
Analysis report saved to results/analysis_report.txt

Analysis complete!
Generated files:
  - results/roofline_plot.png
  - results/performance_comparison.png
  - results/analysis_report.txt
No profiling data found, skipping Nsight analysis

✓ Analysis complete

========================================================================
PHASE 1 COMPLETE!
========================================================================

Generated Files:
  GPU Specifications:
    • results/gpu_specs.txt

  Benchmark Results:
    • results/benchmark_results.csv
    • results/cutlass/ (CUTLASS results)

  Profiling Data:
    • (Profiling data not available)

  Analysis & Visualizations:
    • results/roofline_plot.png
    • results/performance_comparison.png
    • results/analysis_report.txt

Next Steps:
  1. Review results/analysis_report.txt for optimization opportunities
  2. Compare against cuBLAS and CUTLASS baselines
  3. Proceed to Phase 2 optimizations


========================================================================
ALL JOBS COMPLETED!
========================================================================
Wed Nov 12 09:05:42 PM EST 2025
