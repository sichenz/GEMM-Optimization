========================================================================
                    PHASE 1: COMPLETE BASELINE ANALYSIS                
========================================================================

This script will:
  1. Collect GPU specifications
  2. Run GEMM benchmarks (Lab-1, cuBLAS)
  3. Profile Lab-1 kernel with Nsight Compute
  4. Run CUTLASS benchmarks
  5. Generate comprehensive analysis and visualizations

Thu Nov 13 01:46:42 AM EST 2025

========================================================================
STEP 1: Building Project
========================================================================
-- The CXX compiler identification is GNU 13.3.0
-- The CUDA compiler identification is NVIDIA 12.8.93
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version "12.8.93") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Configuring done (5.1s)
-- Generating done (0.4s)
-- Build files have been written to: /scratch/sz4972/GEMM-Optimization/build
[ 16%] Building CUDA object CMakeFiles/gpu_specs.dir/src/gpu_specs.cu.o
[ 50%] Building CUDA object CMakeFiles/benchmark_gemm.dir/src/benchmark_gemm.cu.o
[ 50%] Building CUDA object CMakeFiles/cublas_bench.dir/src/baselines/cublas_bench.cu.o
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 66%] Linking CUDA executable gpu_specs
[ 66%] Built target gpu_specs
[ 83%] Linking CUDA executable cublas_bench
[ 83%] Built target cublas_bench
[100%] Linking CUDA executable benchmark_gemm
[100%] Built target benchmark_gemm
✓ Build successful

========================================================================
STEP 2: GPU Specifications
========================================================================
========================================
GPU Specifications Report
Generated: Nov 13 2025 01:47:04
========================================

========================================
Device 0: Quadro RTX 8000
========================================

=== Basic Information ===
Compute Capability: 7.5
Total Global Memory: 44.48 GB
Memory Clock Rate: 6501.00 MHz
Memory Bus Width: 384 bits

=== Compute Resources ===
Number of SMs: 72
CUDA Cores: 4608 (total)
CUDA Cores per SM: 64
Max Threads per SM: 1024
Max Threads per Block: 1024
Max Block Dimensions: (1024, 1024, 64)
Max Grid Dimensions: (2147483647, 65535, 65535)
Warp Size: 32

=== Memory Hierarchy ===
L2 Cache Size: 6.00 MB
Shared Memory per Block: 48.00 KB
Shared Memory per SM: 64.00 KB
Registers per Block: 65536
Registers per SM: 65536
Constant Memory: 64.00 KB

=== Performance Characteristics ===
Peak Memory Bandwidth: 624.1 GB/s
Estimated FP32 Peak TFLOPS: 14.93 TFLOPS
Base Clock Rate: 1.62 GHz
TensorCore Architecture: Turing
TensorCore FP16 Peak TFLOPS: 65.0 TFLOPS
TensorCore Speedup vs FP32: 4.4x

=== Feature Support ===
Concurrent Kernels: Yes
ECC Enabled: Yes
Unified Addressing: Yes
Managed Memory: Yes
Cooperative Launch: Yes
TensorCore Support: Yes (Compute 7.5)

=== Arithmetic Intensity Analysis ===
FP32 Ridge Point: 23.92 FLOPS/Byte
  (Workloads with AI > 23.92 are compute-bound)
FP16 TensorCore Ridge Point: 104.15 FLOPS/Byte

✓ Results saved to results/gpu_specs.txt
✓ GPU specs collected

========================================================================
STEP 3: GEMM Benchmarks (Lab-1, cuBLAS)
========================================================================
This may take 30-60 minutes...

GEMM Benchmark Suite
====================

GPU: Quadro RTX 8000
Compute Capability: 7.5

         Kernel   DType     M     N     K    Time(ms)      GFLOPSBandwidth(GB/s)
--------------------------------------------------------------------------------

Testing M=128, N=128, K=128
     Lab1_Tiled    FP32   128   128   128       0.013      315.76          14.80
   cuBLAS_SGEMM    FP32   128   128   128       0.009      479.85          22.49
cuBLAS_HGEMM_TensorCore    FP16   128   128   128       0.007      610.92          14.32

Testing M=256, N=256, K=256
     Lab1_Tiled    FP32   256   256   256       0.030     1101.27          25.81
   cuBLAS_SGEMM    FP32   256   256   256       0.013     2521.52          59.10
cuBLAS_HGEMM_TensorCore    FP16   256   256   256       0.010     3512.23          41.16

Testing M=512, N=512, K=512
     Lab1_Tiled    FP32   512   512   512       0.200     1341.73          15.72
   cuBLAS_SGEMM    FP32   512   512   512       0.042     6345.39          74.36
cuBLAS_HGEMM_TensorCore    FP16   512   512   512       0.018    14754.39          86.45

Testing M=1024, N=1024, K=1024
     Lab1_Tiled    FP32  1024  1024  1024       1.461     1469.98           8.61
   cuBLAS_SGEMM    FP32  1024  1024  1024       0.241     8922.63          52.28
cuBLAS_HGEMM_TensorCore    FP16  1024  1024  1024       0.041    52721.24         154.46

Testing M=2048, N=2048, K=2048
     Lab1_Tiled    FP32  2048  2048  2048      10.577     1624.28           4.76
   cuBLAS_SGEMM    FP32  2048  2048  2048       1.345    12772.64          37.42
cuBLAS_HGEMM_TensorCore    FP16  2048  2048  2048       0.195    88268.47         129.30

Testing M=4096, N=4096, K=4096
     Lab1_Tiled    FP32  4096  4096  4096      70.454     1950.76           2.86
   cuBLAS_SGEMM    FP32  4096  4096  4096      10.520    13064.76          19.14
cuBLAS_HGEMM_TensorCore    FP16  4096  4096  4096       1.510    91047.64          66.69

Testing M=8192, N=8192, K=8192
     Lab1_Tiled    FP32  8192  8192  8192     564.731     1946.97           1.43
   cuBLAS_SGEMM    FP32  8192  8192  8192      82.571    13315.98           9.75
cuBLAS_HGEMM_TensorCore    FP16  8192  8192  8192      12.064    91139.28          33.38

Testing M=4096, N=256, K=1024
     Lab1_Tiled    FP32  4096   256  1024       1.249     1719.69          17.63
   cuBLAS_SGEMM    FP32  4096   256  1024       0.186    11536.38         118.29
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.039    54422.89         279.02

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.286     1878.95          11.93
   cuBLAS_SGEMM    FP32  1024  4096   512       0.353    12182.89          77.33
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.065    66095.95         209.78

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.298     1868.90          10.95
   cuBLAS_SGEMM    FP32  2048   512  2048       0.354    12145.19          71.16
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    73391.15         215.01

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.566     1895.78          16.66
   cuBLAS_SGEMM    FP32   512  2048   512       0.094    11476.90         100.87
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.019    56346.65         247.62
Results saved to results/benchmark_results.csv

Benchmark completed!
✓ Benchmarks complete

========================================================================
STEP 4: Nsight Compute Profiling
========================================================================
Profiling Lab-1 kernel for key matrix sizes...

Building profiling executable...
-- The CXX compiler identification is GNU 13.3.0
-- The CUDA compiler identification is NVIDIA 12.8.93
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version "12.8.93") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Configuring done (2.3s)
-- Generating done (0.0s)
-- Build files have been written to: /scratch/sz4972/GEMM-Optimization/build_profile
[ 50%] Building CUDA object CMakeFiles/benchmark_gemm.dir/src/benchmark_gemm.cu.o
nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[100%] Linking CUDA executable benchmark_gemm
[100%] Built target benchmark_gemm
Profiling 512x512x512...
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.040    53887.55         276.28

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.307     1861.97          11.82
   cuBLAS_SGEMM    FP32  1024  4096   512       0.350    12253.97          77.78
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.066    64985.47         206.25

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.314     1856.22          10.88
   cuBLAS_SGEMM    FP32  2048   512  2048       0.354    12144.86          71.16
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    72811.85         213.32

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.572     1876.35          16.49
   cuBLAS_SGEMM    FP32   512  2048   512       0.094    11396.60         100.17
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.031    34868.99         153.23
Results saved to results/benchmark_results.csv

Benchmark completed!
==PROF== Disconnected from process 3462653
  ✓ Profiling 512 succeeded
Profiling 2048x2048x2048...
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.040    53704.28         275.34

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.301     1866.40          11.85
   cuBLAS_SGEMM    FP32  1024  4096   512       0.350    12270.27          77.89
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.066    64908.46         206.01

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.311     1858.57          10.89
   cuBLAS_SGEMM    FP32  2048   512  2048       0.354    12142.50          71.15
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    72888.96         213.54

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.574     1870.33          16.44
   cuBLAS_SGEMM    FP32   512  2048   512       0.094    11368.79          99.92
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.029    36448.44         160.17
Results saved to results/benchmark_results.csv

Benchmark completed!
==PROF== Disconnected from process 3462738
  ✓ Profiling 2048 succeeded
Profiling 4096x4096x4096...
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.040    53445.52         274.01

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.322     1849.95          11.74
   cuBLAS_SGEMM    FP32  1024  4096   512       0.349    12315.19          78.17
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.066    65094.20         206.60

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.309     1860.03          10.90
   cuBLAS_SGEMM    FP32  2048   512  2048       0.356    12078.25          70.77
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    72554.05         212.56

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.575     1868.59          16.42
   cuBLAS_SGEMM    FP32   512  2048   512       0.095    11351.68          99.77
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.031    35131.85         154.39
Results saved to results/benchmark_results.csv

Benchmark completed!
==PROF== Disconnected from process 3462797
  ✓ Profiling 4096 succeeded
✓ Profiling step complete

========================================================================
STEP 5: CUTLASS Benchmarks
========================================================================
Initializing CUTLASS submodule...
Detected GPU compute capability: 75
Running CUTLASS benchmarks...
          cuBLAS: Not run
           cuDNN: Not run

       Arguments: --gemm_kind=universal --m=8192 --n=8192 --k=8192 --A=f16:row --B=f16:row --C=f16:column --D=f16:column  \
                  --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1 --batch_count=1 --raster_order=heuristic  \
                  --runtime_input_datatype_a=invalid --runtime_input_datatype_b=invalid --use_pdl=false --enable_sm90_mixed_dtype_shuffle_test=false  \
                  --swizzle_size=1 --op_class=tensorop --accum=f32 --cta_m=256 --cta_n=128 --cta_k=32 --cluster_m=1 --cluster_n=1  \
                  --cluster_k=1 --cluster_m_fallback=0 --cluster_n_fallback=0 --cluster_k_fallback=0 --stages=2 --warps_m=4  \
                  --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=8 --min_cc=75 --max_cc=1024

           Bytes: 402653184  bytes
           FLOPs: 1099645845504  flops
           FLOPs/Byte: 2731

         Runtime: 11.6314  ms
          Memory: 32.2403 GiB/s

            Math: 94541 GFLOP/s

Wrote results to 'results/cutlass/cutlass_fp32.gemm.csv'
✓ FP32 benchmarks complete
          cuBLAS: Not run
           cuDNN: Not run

       Arguments: --gemm_kind=universal --m=8192 --n=8192 --k=8192 --A=f16:row --B=f16:row --C=f16:column --D=f16:column  \
                  --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1 --batch_count=1 --raster_order=heuristic  \
                  --runtime_input_datatype_a=invalid --runtime_input_datatype_b=invalid --use_pdl=false --enable_sm90_mixed_dtype_shuffle_test=false  \
                  --swizzle_size=1 --op_class=tensorop --accum=f32 --cta_m=256 --cta_n=128 --cta_k=32 --cluster_m=1 --cluster_n=1  \
                  --cluster_k=1 --cluster_m_fallback=0 --cluster_n_fallback=0 --cluster_k_fallback=0 --stages=2 --warps_m=4  \
                  --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=8 --min_cc=75 --max_cc=1024

           Bytes: 402653184  bytes
           FLOPs: 1099645845504  flops
           FLOPs/Byte: 2731

         Runtime: 11.6464  ms
          Memory: 32.1988 GiB/s

            Math: 94419.5 GFLOP/s

Wrote results to 'results/cutlass/cutlass_f16tc.gemm.csv'
✓ FP16 TensorCore benchmarks complete
✓ CUTLASS benchmarks complete

========================================================================
STEP 6: Generating Analysis and Visualizations
========================================================================
Parsed GPU specs from file:
  FP32 Peak: 14930.0 GFLOPS
  FP16 Peak: 65000.0 GFLOPS
  Bandwidth: 624.1 GB/s

Loaded 33 benchmark results
Kernels tested: Lab1_Tiled, cuBLAS_SGEMM, cuBLAS_HGEMM_TensorCore
Matrix sizes: 11

Generating analysis...
Roofline plot saved to results/roofline_plot.png
Performance comparison saved to results/performance_comparison.png
Analysis report saved to results/analysis_report.txt

Analysis complete!
Generated files:
  - results/roofline_plot.png
  - results/performance_comparison.png
  - results/analysis_report.txt
No profiling data found, skipping Nsight analysis

✓ Analysis complete

========================================================================
PHASE 1 COMPLETE!
========================================================================

Generated Files:
  GPU Specifications:
    • results/gpu_specs.txt

  Benchmark Results:
    • results/benchmark_results.csv
    • results/cutlass/ (CUTLASS results)

  Profiling Data:
    • (Profiling data not available)

  Analysis & Visualizations:
    • results/roofline_plot.png
    • results/performance_comparison.png
    • results/analysis_report.txt

Next Steps:
  1. Review results/analysis_report.txt for optimization opportunities
  2. Compare against cuBLAS and CUTLASS baselines
  3. Proceed to Phase 2 optimizations


========================================================================
ALL JOBS COMPLETED!
========================================================================
Thu Nov 13 02:31:13 AM EST 2025
