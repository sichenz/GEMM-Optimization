slurmstepd: error: Prolog hung on node gr003
======================================
Phase 1.1: Complete Baseline Analysis
======================================

Wed Oct 29 04:40:20 PM EDT 2025
======================================
Step 1: Building project...
======================================
-- The CXX compiler identification is GNU 13.3.0
-- The CUDA compiler identification is NVIDIA 12.8.93
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version "12.8.93") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Configuring done (7.1s)
-- Generating done (0.6s)
-- Build files have been written to: /scratch/sz4972/GEMM-Optimization/build
[ 50%] Building CUDA object CMakeFiles/gpu_specs.dir/src/gpu_specs.cu.o
[ 50%] Building CUDA object CMakeFiles/benchmark_gemm.dir/src/benchmark_gemm.cu.o
[ 75%] Linking CUDA executable gpu_specs
[ 75%] Built target gpu_specs
[100%] Linking CUDA executable benchmark_gemm
[100%] Built target benchmark_gemm
Build successful!

======================================
Step 2: Collecting GPU specifications...
======================================
GPU Specifications Report
Generated: Oct 29 2025 16:40:42

========================================
Device 0: Quadro RTX 8000
========================================

=== Basic Information ===
Compute Capability: 7.5
Total Global Memory: 44.48 GB
Memory Clock Rate: 6501.00 MHz
Memory Bus Width: 384 bits

=== Compute Resources ===
Number of SMs: 72
Max Threads per SM: 1024
Max Threads per Block: 1024
Max Block Dimensions: (1024, 1024, 64)
Max Grid Dimensions: (2147483647, 65535, 65535)
Warp Size: 32

=== Memory Hierarchy ===
L2 Cache Size: 6.00 MB
Shared Memory per Block: 48.00 KB
Shared Memory per SM: 64.00 KB
Registers per Block: 65536
Registers per SM: 65536
Constant Memory: 64.00 KB

=== Performance Characteristics ===
Peak Memory Bandwidth: 624.1 GB/s
Estimated FP32 Peak TFLOPS: -2.25 TFLOPS
TensorCore FP16 Peak TFLOPS: 65.00 TFLOPS
TensorCore Speedup vs FP32: -28.9x
Clock Rate: 1620.0 MHz

=== Feature Support ===
Concurrent Kernels: Yes
ECC Enabled: Yes
Unified Addressing: Yes
Managed Memory: Yes
Cooperative Launch: Yes
TensorCore Support: Yes (Compute 7.5)

Results saved to results/gpu_specs.txt
GPU specs complete!

======================================
Step 3: Running GEMM benchmarks...
This will take 30-60 minutes...
======================================
GEMM Benchmark Suite
====================

GPU: Quadro RTX 8000
Compute Capability: 7.5

         Kernel   DType     M     N     K    Time(ms)      GFLOPSBandwidth(GB/s)
--------------------------------------------------------------------------------

Testing M=128, N=128, K=128
     Lab1_Tiled    FP32   128   128   128       0.014      306.57          14.37
   cuBLAS_SGEMM    FP32   128   128   128       0.008      512.00          24.00
cuBLAS_HGEMM_TensorCore    FP16   128   128   128       0.008      504.03          11.81

Testing M=256, N=256, K=256
     Lab1_Tiled    FP32   256   256   256       0.028     1190.35          27.90
   cuBLAS_SGEMM    FP32   256   256   256       0.012     2693.15          63.12
cuBLAS_HGEMM_TensorCore    FP16   256   256   256       0.009     3798.50          44.51

Testing M=512, N=512, K=512
     Lab1_Tiled    FP32   512   512   512       0.196     1366.21          16.01
   cuBLAS_SGEMM    FP32   512   512   512       0.044     6164.92          72.25
cuBLAS_HGEMM_TensorCore    FP16   512   512   512       0.018    14782.99          86.62

Testing M=1024, N=1024, K=1024
     Lab1_Tiled    FP32  1024  1024  1024       1.461     1469.94           8.61
   cuBLAS_SGEMM    FP32  1024  1024  1024       0.241     8925.95          52.30
cuBLAS_HGEMM_TensorCore    FP16  1024  1024  1024       0.041    53008.58         155.30

Testing M=2048, N=2048, K=2048
     Lab1_Tiled    FP32  2048  2048  2048       9.777     1757.13           5.15
   cuBLAS_SGEMM    FP32  2048  2048  2048       1.315    13064.29          38.27
cuBLAS_HGEMM_TensorCore    FP16  2048  2048  2048       0.190    90254.67         132.21

Testing M=4096, N=4096, K=4096
     Lab1_Tiled    FP32  4096  4096  4096      70.410     1951.99           2.86
   cuBLAS_SGEMM    FP32  4096  4096  4096      10.539    13040.72          19.10
cuBLAS_HGEMM_TensorCore    FP16  4096  4096  4096       1.506    91279.84          66.86

Testing M=8192, N=8192, K=8192
     Lab1_Tiled    FP32  8192  8192  8192     563.948     1949.67           1.43
   cuBLAS_SGEMM    FP32  8192  8192  8192      83.249    13207.47           9.67
cuBLAS_HGEMM_TensorCore    FP16  8192  8192  8192      12.047    91270.94          33.42

Testing M=4096, N=256, K=1024
     Lab1_Tiled    FP32  4096   256  1024       1.327     1618.05          16.59
   cuBLAS_SGEMM    FP32  4096   256  1024       0.208    10305.26         105.67
cuBLAS_HGEMM_TensorCore    FP16  4096   256  1024       0.041    52212.61         267.69

Testing M=1024, N=4096, K=512
     Lab1_Tiled    FP32  1024  4096   512       2.505     1714.77          10.88
   cuBLAS_SGEMM    FP32  1024  4096   512       0.350    12254.58          77.79
cuBLAS_HGEMM_TensorCore    FP16  1024  4096   512       0.065    65741.44         208.65

Testing M=2048, N=512, K=2048
     Lab1_Tiled    FP32  2048   512  2048       2.299     1868.28          10.95
   cuBLAS_SGEMM    FP32  2048   512  2048       0.356    12071.62          70.73
cuBLAS_HGEMM_TensorCore    FP16  2048   512  2048       0.059    73201.02         214.46

Testing M=512, N=2048, K=512
     Lab1_Tiled    FP32   512  2048   512       0.565     1901.98          16.72
   cuBLAS_SGEMM    FP32   512  2048   512       0.094    11394.08         100.14
cuBLAS_HGEMM_TensorCore    FP16   512  2048   512       0.019    56713.31         249.23
Results saved to results/benchmark_results.csv

Benchmark completed!
Benchmarks complete!

======================================
Step 4: Generating roofline analysis...
======================================
/scratch/sz4972/GEMM-Optimization/src/roofline_analysis.py:97: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.
  plt.tight_layout()

Loaded 33 benchmark results
Kernels tested: Lab1_Tiled, cuBLAS_SGEMM, cuBLAS_HGEMM_TensorCore
Matrix sizes: 11

Generating analysis...
Roofline plot saved to results/roofline_plot.png
Performance comparison saved to results/performance_comparison.png
Analysis report saved to results/analysis_report.txt

Analysis complete!
Generated files:
  - results/roofline_plot.png
  - results/performance_comparison.png
  - results/analysis_report.txt
Analysis complete!

======================================
Phase 1.1 Complete!
======================================

Results available in:
  - results/gpu_specs.txt
  - results/benchmark_results.csv
  - results/roofline_plot.png
  - results/performance_comparison.png
  - results/analysis_report.txt

All jobs completed! Check results/ directory for outputs.
