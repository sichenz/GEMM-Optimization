================================================================================
GEMM PERFORMANCE ANALYSIS REPORT
================================================================================

GPU SPECIFICATIONS:
----------------------------------------
Peak FP32 Performance: -2250.00 GFLOPS
Peak FP16 TensorCore Performance: 65000.00 GFLOPS
Peak Memory Bandwidth: 624.10 GB/s
TensorCore Speedup (theoretical): -28.9x

PERFORMANCE SUMMARY BY KERNEL:
----------------------------------------

Lab1_Tiled:
  Average GFLOPS: 1554.09
  Max GFLOPS: 1951.99
  Min GFLOPS: 306.57
  Average Bandwidth: 11.95 GB/s
  Average AI: 295.35 FLOPS/Byte
  Average Efficiency: -69.07% of peak

cuBLAS_SGEMM:
  Average GFLOPS: 9421.28
  Max GFLOPS: 13207.47
  Min GFLOPS: 512.00
  Average Bandwidth: 57.55 GB/s
  Average AI: 295.35 FLOPS/Byte
  Average Efficiency: -418.72% of peak

cuBLAS_HGEMM_TensorCore:
  Average GFLOPS: 53887.99
  Max GFLOPS: 91279.84
  Min GFLOPS: 504.03
  Average Bandwidth: 133.71 GB/s
  Average AI: 590.70 FLOPS/Byte
  Average Efficiency: 82.90% of peak

================================================================================
LAB-1 vs cuBLAS COMPARISON:
----------------------------------------

Matrix 128x128x128:
  Lab-1: 306.57 GFLOPS
  cuBLAS: 512.00 GFLOPS
  cuBLAS Speedup: 1.67x
  Lab-1 Efficiency: 59.88% of cuBLAS

Matrix 256x256x256:
  Lab-1: 1190.35 GFLOPS
  cuBLAS: 2693.15 GFLOPS
  cuBLAS Speedup: 2.26x
  Lab-1 Efficiency: 44.20% of cuBLAS

Matrix 512x512x512:
  Lab-1: 1366.21 GFLOPS
  cuBLAS: 6164.92 GFLOPS
  cuBLAS Speedup: 4.51x
  Lab-1 Efficiency: 22.16% of cuBLAS

Matrix 1024x1024x1024:
  Lab-1: 1469.94 GFLOPS
  cuBLAS: 8925.95 GFLOPS
  cuBLAS Speedup: 6.07x
  Lab-1 Efficiency: 16.47% of cuBLAS

Matrix 2048x2048x2048:
  Lab-1: 1757.13 GFLOPS
  cuBLAS: 13064.29 GFLOPS
  cuBLAS Speedup: 7.44x
  Lab-1 Efficiency: 13.45% of cuBLAS

Matrix 4096x4096x4096:
  Lab-1: 1951.99 GFLOPS
  cuBLAS: 13040.72 GFLOPS
  cuBLAS Speedup: 6.68x
  Lab-1 Efficiency: 14.97% of cuBLAS

Matrix 8192x8192x8192:
  Lab-1: 1949.67 GFLOPS
  cuBLAS: 13207.47 GFLOPS
  cuBLAS Speedup: 6.77x
  Lab-1 Efficiency: 14.76% of cuBLAS

Matrix 4096x256x1024:
  Lab-1: 1618.05 GFLOPS
  cuBLAS: 10305.26 GFLOPS
  cuBLAS Speedup: 6.37x
  Lab-1 Efficiency: 15.70% of cuBLAS

Matrix 1024x4096x512:
  Lab-1: 1714.77 GFLOPS
  cuBLAS: 12254.58 GFLOPS
  cuBLAS Speedup: 7.15x
  Lab-1 Efficiency: 13.99% of cuBLAS

Matrix 2048x512x2048:
  Lab-1: 1868.28 GFLOPS
  cuBLAS: 12071.62 GFLOPS
  cuBLAS Speedup: 6.46x
  Lab-1 Efficiency: 15.48% of cuBLAS

Matrix 512x2048x512:
  Lab-1: 1901.98 GFLOPS
  cuBLAS: 11394.08 GFLOPS
  cuBLAS Speedup: 5.99x
  Lab-1 Efficiency: 16.69% of cuBLAS

================================================================================
TENSORCORE ANALYSIS (FP16 vs FP32):
----------------------------------------

Matrix 128x128x128:
  FP32: 512.00 GFLOPS
  FP16 TensorCore: 504.03 GFLOPS
  TensorCore Speedup: 0.98x

Matrix 256x256x256:
  FP32: 2693.15 GFLOPS
  FP16 TensorCore: 3798.50 GFLOPS
  TensorCore Speedup: 1.41x

Matrix 512x512x512:
  FP32: 6164.92 GFLOPS
  FP16 TensorCore: 14782.99 GFLOPS
  TensorCore Speedup: 2.40x

Matrix 1024x1024x1024:
  FP32: 8925.95 GFLOPS
  FP16 TensorCore: 53008.58 GFLOPS
  TensorCore Speedup: 5.94x

Matrix 2048x2048x2048:
  FP32: 13064.29 GFLOPS
  FP16 TensorCore: 90254.67 GFLOPS
  TensorCore Speedup: 6.91x

Matrix 4096x4096x4096:
  FP32: 13040.72 GFLOPS
  FP16 TensorCore: 91279.84 GFLOPS
  TensorCore Speedup: 7.00x

Matrix 8192x8192x8192:
  FP32: 13207.47 GFLOPS
  FP16 TensorCore: 91270.94 GFLOPS
  TensorCore Speedup: 6.91x

Matrix 4096x256x1024:
  FP32: 10305.26 GFLOPS
  FP16 TensorCore: 52212.61 GFLOPS
  TensorCore Speedup: 5.07x

Matrix 1024x4096x512:
  FP32: 12254.58 GFLOPS
  FP16 TensorCore: 65741.44 GFLOPS
  TensorCore Speedup: 5.36x

Matrix 2048x512x2048:
  FP32: 12071.62 GFLOPS
  FP16 TensorCore: 73201.02 GFLOPS
  TensorCore Speedup: 6.06x

Matrix 512x2048x512:
  FP32: 11394.08 GFLOPS
  FP16 TensorCore: 56713.31 GFLOPS
  TensorCore Speedup: 4.98x

================================================================================
OPTIMIZATION OPPORTUNITIES:
----------------------------------------

1. Performance Gap:
   Current Lab-1 average: 1554.09 GFLOPS
   cuBLAS average: 9421.28 GFLOPS
   Gap to close: 7867.19 GFLOPS (83.5%)

2. TensorCore Opportunity:
   Potential speedup with TensorCores: 34.68x
   Potential performance: 53887.99 GFLOPS

3. Memory Efficiency:
   Current bandwidth utilization: 1.9%
   → Focus on memory access patterns and coalescing

4. Arithmetic Intensity:
   Average AI: 295.35 FLOPS/Byte
   Ridge point (compute-bound threshold): -3.61 FLOPS/Byte
   → Compute-bound: optimize computation efficiency

================================================================================
