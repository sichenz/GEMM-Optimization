========================================
GPU Specifications Report
Generated: Dec 11 2025 00:51:12
========================================

========================================
Device 0: NVIDIA H100 80GB HBM3
========================================

=== Basic Information ===
Compute Capability: 9.0
Total Global Memory: 79.11 GB
Memory Clock Rate: 2619.00 MHz
Memory Bus Width: 5120 bits

=== Compute Resources ===
Number of SMs: 132
CUDA Cores: 16896 (total)
CUDA Cores per SM: 128
Max Threads per SM: 2048
Max Threads per Block: 1024
Max Block Dimensions: (1024, 1024, 64)
Max Grid Dimensions: (2147483647, 65535, 65535)
Warp Size: 32

=== Memory Hierarchy ===
L2 Cache Size: 50.00 MB
Shared Memory per Block: 48.00 KB
Shared Memory per SM: 228.00 KB
Registers per Block: 65536
Registers per SM: 65536
Constant Memory: 64.00 KB

=== Performance Characteristics ===
Peak Memory Bandwidth: 3352.3 GB/s
Estimated FP32 Peak TFLOPS: 66.91 TFLOPS
Base Clock Rate: 1.98 GHz
TensorCore Architecture: Hopper
TensorCore FP16 Peak TFLOPS: 989.0 TFLOPS
TensorCore Speedup vs FP32: 14.8x

=== Feature Support ===
Concurrent Kernels: Yes
ECC Enabled: Yes
Unified Addressing: Yes
Managed Memory: Yes
Cooperative Launch: Yes
TensorCore Support: Yes (Compute 9.0)

=== Arithmetic Intensity Analysis ===
FP32 Ridge Point: 19.96 FLOPS/Byte
  (Workloads with AI > 19.96 are compute-bound)
  (Workloads with AI < 19.96 are memory-bound)

